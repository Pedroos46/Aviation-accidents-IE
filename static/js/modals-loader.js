document.write(`

<!-- Tools Modal -->
<div class="modal fade" id="ToolsModalLong" tabindex="-1" role="dialog" aria-labelledby="ToolsModalLong"
     aria-hidden="true">
    <div class="modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="ToolsModalLongTitle">Tools</h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true">&times;</span>
                </button>
            </div>
            <div class="modal-body ">
                 <div class="card-columns">
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../../static/logos/Pytorch_logo.png" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">PyTorch</p>
                        </div>
                    </div>
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../logos/colab_logo.png" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">Google Colab</p>
                        </div>
                    </div>
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../logos/hug_face.jpeg" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">BERT</p>
                        </div>
                    </div>
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../logos/Flask_logo.png" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">Google Colab</p>
                        </div>
                    </div>
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../logos/PyCharm_Icon.png" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">PyCharm IDE</p>
                        </div>
                    </div>
            
                    <div class="card tool-padding" style="width: 10rem;">
                        <img class="card-img-top img-padding" src="../logos/QuillBot_logo.jpeg" alt="Card image cap">
                        <div class="card-body text-center">
                            <p class="card-text">QuillBot</p>
                        </div>
                    </div>

    </div>    
            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>

<!-- References Modal -->
<div class="modal fade" id="ReferenceModalLong" tabindex="-1" role="dialog" aria-labelledby="ReferenceModalLong"
     aria-hidden="true">
    <div class="modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="ReferenceModalLongTitle">References</h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true">&times;</span>
                </button>
            </div>
            <div class="modal-body">


                <ul>
                    <li>
                        <p>BERT <i>WordPiece Tokenizer Tutorial.</i> (n.d.).</p>
                        <ul>
                            <li>
                                <p>Towards Data Science. Retrieved December 6, 2021, from
                                    <a href="https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-BERT-f505d97dddbb">
                                        https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-BERT-f505d97dddbb</a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>Deeplizard. (2017, Nov 22).</p>
                        <ul>
                            <li>
                                <p><i>Fine-Tuning A Neural Network Explained.</i>Machine Learning & Deep Learning
                                    Fundamentals. Retrieved Des 2, 2021, from
                                    <a href="https://deeplizard.com/learn/video/5T-iXNNiwI">
                                        https://deeplizard.com/learn/video/5T-iXNNiwI
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>Devlin, J. (2018, Oct 11).</p>
                        <ul>
                            <li>
                                <p><i>BERT: Pre-training of Deep Bidirectional Transformers for Language
                                    Understanding.</i>
                                    Arxiv.org. Retrieved Des 2, 2021, from
                                    <a href="https://arxiv.org/abs/1810.04805v2">
                                        https://arxiv.org/abs/1810.04805v2
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>HugginFace. (2020).</p>
                        <ul>
                            <li>
                                <p><i>BERT.</i> Retrieved Des 2, 2021, from
                                <a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForTokenClassification">
                                        https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForTokenClassification
                                </a>
                                </p>
                            </li>
                        </ul>
                    </li>


                    <li>
                        <p><i>Inside–outside–beginning (tagging).</i>(n.d.).</p>
                        <ul>
                            <li>
                                <p>Inside–outside–beginning (tagging).
                                    <a href="https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)">
                                        https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>Jagtap, R. (n.d.).</p>
                        <ul>
                            <li>
                                <p><i>BERT: Pre-Training of Transformers for Language Understanding. </i>
                                    <a href="https://medium.com/swlh/BERT-pre-training-of-transformers-for-language-understanding-5214fba4a9af">
                                        https://medium.com/swlh/BERT-pre-training-of-transformers-for-language-understanding-5214fba4a9af
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>Saggion, H., & Szasz, S. (2012).</p>
                        <ul>
                            <li>
                                <p><i>The CONCISUS Corpus of Event Summaries.</i>ACL Anthology. Retrieved Des 2, 2021,
                                    from
                                    <a href="https://aclanthology.org/L12-1372/">
                                        https://aclanthology.org/L12-1372/
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                    <li>
                        <p>Understanding BERT <i> Word Embeddings | by Dharti Dhami.</i>(n.d.).</p>
                        <ul>
                            <li>
                                <p> Medium. Retrieved December 6, 2021, from
                                    <a href="https://medium.com/@dhartidhami/understanding-BERT-word-embeddings-7dc4d2ea54ca">
                                        https://medium.com/@dhartidhami/understanding-BERT-word-embeddings-7dc4d2ea54ca
                                    </a>
                                </p>
                            </li>
                        </ul>
                    </li>

                </ul>


            </div>
            <div class="modal-footer">
                <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>

`);
    
